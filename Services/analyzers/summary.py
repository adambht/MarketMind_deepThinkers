# analyzers/summary.py

# Sentiment categorization
def get_sentiment_label(sentiment_score):
    if sentiment_score <= -0.6:
        return "ðŸ˜¡ Very Negative"
    elif sentiment_score <= -0.3:
        return "ðŸ™ Negative"
    elif sentiment_score < 0.3:
        return "ðŸ˜ Neutral"
    elif sentiment_score < 0.6:
        return "ðŸ™‚ Positive"
    else:
        return "ðŸ¤© Very Positive"

# Toxicity categorization
def get_toxicity_label(toxic_percentage):
    if toxic_percentage <= 1:
        return "ðŸŸ¢ Very Low Toxicity"
    elif toxic_percentage <= 5:
        return "ðŸŸ¡ Low Toxicity"
    elif toxic_percentage <= 15:
        return "ðŸŸ  Moderate Toxicity"
    elif toxic_percentage <= 30:
        return "ðŸ”´ High Toxicity"
    else:
        return "âš« Very High Toxicity"

# Generate the final plain-text summary
def generate_summary_response(avg_sentiment, toxic_percentage):
    sentiment_label = get_sentiment_label(avg_sentiment)
    toxicity_label = get_toxicity_label(toxic_percentage)

    summary = (
        f"Overall, the reviews are {sentiment_label} (average sentiment score: {avg_sentiment:.2f}). "
        f"Toxicity level: {toxicity_label} ({toxic_percentage:.2f}% of the reviews). "
    )

    if toxic_percentage <= 1:
        summary += "ðŸŸ¢ Toxicity is very rare and does not significantly affect the general perception."
    elif toxic_percentage <= 5:
        summary += "ðŸŸ¡ A small portion of reviews show toxicity, but the general sentiment remains positive."
    elif toxic_percentage <= 15:
        summary += "ðŸŸ  Moderate toxicity detected. Attention may be needed to address certain negative feedback."
    elif toxic_percentage <= 30:
        summary += "ðŸ”´ High toxicity observed. Sentiment may be heavily influenced by toxic reviews."
    else:
        summary += "âš« Very high toxicity found. Reputation risk is critical â€” immediate actions are recommended."

    return {"summary": summary}
